config:
  user: marawan.gamal # your username to check slurm status
  max_jobs: 10 # maximum number of jobs to run in parallel
  max_gpus: 10 # maximum number of GPUs to use in parallel

common_preamble_declarations:
  - "#!/bin/bash"
  - "#SBATCH --output=slurm/slurm-%j.out"
  - "#SBATCH --error=slurm/slurm-%j.err"

common_preamble_runs:
  # 1. Load the required modules
  - module load python/3.9

  # 2. Load your environment
  - source /home/mila/m/marawan.gamal/scratch/tjdnet/.venv/bin/activate

  # # 3. Copy your dataset on the compute node
  # - cp -r /home/mila/m/marawan.gamal/.cache/huggingface $SLURM_TMPDIR/huggingface

# Groups are nodes
# - jobs within a group are run in parallel
groups:

  # Basic 
  - name: gsm8k
    preamble:
      - "#SBATCH --partition=dev-short-unkillable"
      - "#SBATCH --gres=gpu:a100l:4"
      - "#SBATCH --cpus-per-task=12"
      - "#SBATCH --mem=128G"
      - "#SBATCH --nodes=1"
      - "#SBATCH --time=3:00:00"

    paralleljobs:
      #  baseline
      - accelerate launch --use_fsdp --config_file configs/fsdp/fsdp_4gpus.yaml train.py --dataset gsm8k --model meta-llama/Llama-3.2-3B-Instruct --epochs 5 --batch_size 8 --seq_len 128 --lr 5e-5 --model_head base --horizon 1 --horizon_eval 1
      
      # ours (cp)
      - accelerate launch --use_fsdp --config_file configs/fsdp/fsdp_4gpus.yaml train.py --dataset gsm8k --model meta-llama/Llama-3.2-3B-Instruct --epochs 5 --batch_size 8 --seq_len 128 --lr 5e-5 --model_head cp --hidden_dim 8192 --horizon 2 --horizon_eval 2 --rank 8
      - accelerate launch --use_fsdp --config_file configs/fsdp/fsdp_4gpus.yaml train.py --dataset gsm8k --model meta-llama/Llama-3.2-3B-Instruct --epochs 5 --batch_size 8 --seq_len 128 --lr 5e-5 --model_head cp --hidden_dim 8192 --horizon 3 --horizon_eval 3 --rank 8
      
      #  ours (mps)
      - accelerate launch --use_fsdp --config_file configs/fsdp/fsdp_4gpus.yaml train.py --dataset gsm8k --model meta-llama/Llama-3.2-3B-Instruct --epochs 5 --batch_size 8 --seq_len 128 --lr 5e-5 --model_head mps --hidden_dim 5120 --horizon 2 --horizon_eval 2 --rank 8
      - accelerate launch --use_fsdp --config_file configs/fsdp/fsdp_4gpus.yaml train.py --dataset gsm8k --model meta-llama/Llama-3.2-3B-Instruct --epochs 5 --batch_size 8 --seq_len 128 --lr 5e-5 --model_head mps --hidden_dim 5120 --horizon 3 --horizon_eval 3 --rank 8
      
      # mutli-token-prediction (fb paper)
      - accelerate launch --use_fsdp --config_file configs/fsdp/fsdp_4gpus.yaml train.py --dataset gsm8k --model meta-llama/Llama-3.2-3B-Instruct --epochs 5 --batch_size 8 --seq_len 128 --lr 5e-5 --model_head cpo --hidden_dim 5120 --horizon 2 --horizon_eval 2 --rank 1 
      - accelerate launch --use_fsdp --config_file configs/fsdp/fsdp_4gpus.yaml train.py --dataset gsm8k --model meta-llama/Llama-3.2-3B-Instruct --epochs 5 --batch_size 8 --seq_len 128 --lr 5e-5 --model_head cpo --hidden_dim 5120 --horizon 3 --horizon_eval 3 --rank 1
      
      # tensorized mutli-token-prediction (oslo paper)
      - accelerate launch --use_fsdp --config_file configs/fsdp/fsdp_4gpus.yaml train.py --dataset gsm8k --model meta-llama/Llama-3.2-3B-Instruct --epochs 5 --batch_size 8 --seq_len 128 --lr 5e-5 --model_head cpo --hidden_dim 2048 --horizon 2 --horizon_eval 2 --rank 8 
      - accelerate launch --use_fsdp --config_file configs/fsdp/fsdp_4gpus.yaml train.py --dataset gsm8k --model meta-llama/Llama-3.2-3B-Instruct --epochs 5 --batch_size 8 --seq_len 128 --lr 5e-5 --model_head cpo --hidden_dim 2048 --horizon 3 --horizon_eval 3 --rank 8 

  # - name: gsm8k-eval
  #   # Run eval for every experiment in the `checkpoints` directory
  #   paralleljobs: find checkpoints -mindepth 1 -maxdepth 1 -type d | sed 's/^/python scripts\/eval_acc.py -c checkpoint /'

  # - name: gsm8k-eval-ss
  #   # Run eval for every experiment in the `checkpoints` directory -- with speculative sampling. Results will be saved in each experiment directory.
  #   paralleljobs: find checkpoints -mindepth 1 -maxdepth 1 -type d | sed 's/^/python scripts\/eval_acc.py -c checkpoint --use_speculative_sampling /'

  # - name: gsm8k-eval-bl
  #   # Run eval for hf model -- using only few-shot learning. Results will be saved under `results/eval_acc-bl`.
  #   paralleljobs: python scripts/eval_acc_bl.py --model meta-llama/Llama-3.2-3B-Instruct' --dataset gsm8k --use_few_shot


# tree:
#   - gsm8k:
#     - gsm8k-eval
#     - gsm8k-eval-ss
