config:
  user: marawan.gamal # your username to check slurm status
  max_jobs: 10 # maximum number of jobs to run in parallel
  max_gpus: 10 # maximum number of GPUs to use in parallel

common_preamble_declarations:
  - "#!/bin/bash"
  - "#SBATCH --output=slurm/slurm-%j.out"
  - "#SBATCH --error=slurm/slurm-%j.err"

common_preamble_runs:
  # 1. Load the required modules
  - module load python/3.9

  # 2. Load your environment
  - source /home/mila/m/marawan.gamal/scratch/prod/tjdnet/.venv/bin/activate

  # # 3. Copy your dataset on the compute node
  # - cp -r /home/mila/m/marawan.gamal/.cache/huggingface $SLURM_TMPDIR/huggingface

# Groups are nodes
# - jobs within a group are run in parallel
groups:

  # ====== Main experiments 
  - name: gsm8k
    preamble:
      - "#SBATCH --partition=short-unkillable"
      - "#SBATCH --gres=gpu:a100l:4"
      - "#SBATCH --cpus-per-task=12"
      - "#SBATCH --mem=128G"
      - "#SBATCH --nodes=1"
      - "#SBATCH --time=3:00:00"

    paralleljobs:
      #  baseline
      - accelerate launch --use_fsdp --config_file configs/fsdp/fsdp_4gpus.yaml train.py --dataset gsm8k --model meta-llama/Llama-3.2-3B-Instruct --epochs 5 --batch_size 8 --seq_len 128 --lr 5e-5 --model_head base --horizon 1 --horizon_eval 1
      
      # ours (cp)
      - accelerate launch --use_fsdp --config_file configs/fsdp/fsdp_4gpus.yaml train.py --dataset gsm8k --model meta-llama/Llama-3.2-3B-Instruct --epochs 5 --batch_size 8 --seq_len 128 --lr 5e-5 --model_head cp --hidden_dim 8192 --horizon 2 --horizon_eval 2 --rank 8
      - accelerate launch --use_fsdp --config_file configs/fsdp/fsdp_4gpus.yaml train.py --dataset gsm8k --model meta-llama/Llama-3.2-3B-Instruct --epochs 5 --batch_size 8 --seq_len 128 --lr 5e-5 --model_head cp --hidden_dim 8192 --horizon 3 --horizon_eval 3 --rank 2 --use_memory_efficient_loss
      - accelerate launch --use_fsdp --config_file configs/fsdp/fsdp_4gpus.yaml train.py --dataset gsm8k --model meta-llama/Llama-3.2-3B-Instruct --epochs 5 --batch_size 8 --seq_len 128 --lr 5e-5 --model_head cp --hidden_dim 8192 --horizon 4 --horizon_eval 4 --rank 2 --use_memory_efficient_loss
      
      #  ours (mps)
      - accelerate launch --use_fsdp --config_file configs/fsdp/fsdp_4gpus.yaml train.py --dataset gsm8k --model meta-llama/Llama-3.2-3B-Instruct --epochs 5 --batch_size 8 --seq_len 128 --lr 5e-5 --model_head mps --hidden_dim 5120 --horizon 2 --horizon_eval 2 --rank 8
      - accelerate launch --use_fsdp --config_file configs/fsdp/fsdp_4gpus.yaml train.py --dataset gsm8k --model meta-llama/Llama-3.2-3B-Instruct --epochs 5 --batch_size 8 --seq_len 128 --lr 5e-5 --model_head mps --hidden_dim 5120 --horizon 3 --horizon_eval 3 --rank 8
      
      # mutli-token-prediction (fb paper)
      - accelerate launch --use_fsdp --config_file configs/fsdp/fsdp_4gpus.yaml train.py --dataset gsm8k --model meta-llama/Llama-3.2-3B-Instruct --epochs 5 --batch_size 8 --seq_len 128 --lr 5e-5 --model_head cpo --hidden_dim 5120 --horizon 2 --horizon_eval 2 --rank 1 
      - accelerate launch --use_fsdp --config_file configs/fsdp/fsdp_4gpus.yaml train.py --dataset gsm8k --model meta-llama/Llama-3.2-3B-Instruct --epochs 5 --batch_size 8 --seq_len 128 --lr 5e-5 --model_head cpo --hidden_dim 5120 --horizon 3 --horizon_eval 3 --rank 1
      
      # tensorized mutli-token-prediction (oslo paper)
      - accelerate launch --use_fsdp --config_file configs/fsdp/fsdp_4gpus.yaml train.py --dataset gsm8k --model meta-llama/Llama-3.2-3B-Instruct --epochs 5 --batch_size 8 --seq_len 128 --lr 5e-5 --model_head cpo --hidden_dim 2048 --horizon 2 --horizon_eval 2 --rank 8 
      - accelerate launch --use_fsdp --config_file configs/fsdp/fsdp_4gpus.yaml train.py --dataset gsm8k --model meta-llama/Llama-3.2-3B-Instruct --epochs 5 --batch_size 8 --seq_len 128 --lr 5e-5 --model_head cpo --hidden_dim 2048 --horizon 3 --horizon_eval 3 --rank 8 


  # ====== Grid search
  # - name: stemp
  #   paralleljobs: 
  #     - bash scripts/plots/gpt2_cp_grid/train_gpt2_cp_grid.sh

  # # ====== Evaluation
  # - name: gsm8k-eval-small
  #   # Run eval for every experiment in the `checkpoints` directory
  #   # paralleljobs: "find checkpoints -mindepth 1 -maxdepth 1 -type d | xargs -I@ sh -c \"echo 'Evaluating @'; python scripts/eval_acc.py --max_num_samples 50 -c @ 2>&1 | tee @/eval_results.log\""



# tree:
#   - gsm8k:
#     - gsm8k-eval
#     - gsm8k-eval-ss
