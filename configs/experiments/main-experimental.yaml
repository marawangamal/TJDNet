# Define reusable preamble templates
preambles:
  # Common base preamble
  base:
    - "#!/bin/bash"
    - "#SBATCH --partition=short-unkillable"
    - "#SBATCH --output=slurm/slurm-%j.out"
    - "#SBATCH --error=slurm/slurm-%j.err"

  # Training job preamble
  train:
    - "#SBATCH --gres=gpu:a100l:4"
    - "#SBATCH --cpus-per-task=12"
    - "#SBATCH --mem=128G"
    - "#SBATCH --time=3:00:00"

  # Evaluation job preamble  
  eval:
    - "#SBATCH --gres=gpu:a100l:1"
    - "#SBATCH --cpus-per-task=4"
    - "#SBATCH --mem=32G"
    - "#SBATCH --time=1:00:00"

  # Evaluation job preamble  
  report:
    - "#SBATCH --gres=gpu:a100l:1"
    - "#SBATCH --cpus-per-task=4"
    - "#SBATCH --mem=32G"
    - "#SBATCH --time=1:00:00"

groups:
  # Question: "Does umel=true match umel=false performance?"
  # Answer: ...
  - name: verification-use_memory_efficient_loss
    sequentialjobs:
      - paralleljobs:
        - sequentialjobs: 
          # e.g., 3-layer deep jobrunner_id = aaa-bbb-ccc
          - job:
              preamble: train
              command: accelerate launch --use_fsdp --config_file configs/fsdp/fsdp_4gpus.yaml train.py --dataset gsm8k --model meta-llama/Llama-3.2-3B-Instruct --epochs 5 --batch_size 8 --seq_len 128 --lr 5e-5 --model_head cp --hidden_dim 8192 --horizon 2 --horizon_eval 2 --rank 2 --jobrunner_id {{EXP_ID}} --tag {{GROUP_NAME}}
          - job:
              preamble: eval
              command: python scripts/eval_acc.py --jobrunner_id {{EXP_ID}}
        - sequentialjobs: 
          - job:
              preamble: train
              command: accelerate launch --use_fsdp --config_file configs/fsdp/fsdp_4gpus.yaml train.py --dataset gsm8k --model meta-llama/Llama-3.2-3B-Instruct --epochs 5 --batch_size 8 --seq_len 128 --lr 5e-5 --model_head cp --hidden_dim 8192 --horizon 2 --horizon_eval 2 --rank 2 --use_memory_efficient_loss --jobrunner_id {{EXP_ID}} --tag {{GROUP_NAME}}
          - job:
              preamble: eval
              command: python scripts/eval_acc.py --jobrunner_id {{EXP_ID}}
      # e.g, 1-layer deep jobrunner_id = aaa
      - job:
          preamble: report
          # might not need to pass jobrunner_id, tag the umel.py should just lookup exps relevant to it.
          command: python scripts/plots/umel.py --jobrunner_id {{EXP_ID}} --tag {{GROUP_NAME}}

  - name: tuning-hidden_dim
    # Question: "Does the hidden_dim affect the performance of the model?"
    # Answer: ...
    # Question: "Does the lr need re-tuning when changing the hidden_dim?"
    # Answer: ...
    sequentialjobs:
      - parallellsweep:
        sweep:
          - name: hidden_dim
            values: [2, 4, 8]
          - lr:
            values: [1e-5, 5e-5, 1e-4]
        sequentialjobs:
          - job:
              preamble: train
              command: accelerate launch --use_fsdp --config_file configs/fsdp/fsdp_4gpus.yaml train.py --dataset gsm8k --model meta-llama/Llama-3.2-3B-Instruct --epochs 5 --batch_size 8 --seq_len 128 --lr {{lr}} --model_head cp --hidden_dim {{hidden_dim}} --horizon 2 --horizon_eval 2 --rank 2 --jobrunner_id {{EXP_ID}} --tag {{GROUP_NAME}}
          - job:
              preamble: eval
              command: python scripts/eval_acc.py --jobrunner_id {{EXP_ID}}
      - job:
          preamble: report
          command: python scripts/plots/hd.py --jobrunner_id {{EXP_ID}}
