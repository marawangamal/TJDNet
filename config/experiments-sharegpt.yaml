# test.yaml
preambles:

  gpu4:
    - "#!/bin/bash"
    - "#SBATCH --output=slurm/slurm-%j.out"
    - "#SBATCH --error=slurm/slurm-%j.err"   
    # MAIN >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
    - "#SBATCH --partition=short-unkillable"
    - "#SBATCH --gres=gpu:a100l:4"
    - "#SBATCH --mem=128G"
    - "#SBATCH --time=3:00:00"
    - "#SBATCH --cpus-per-task=12"
    # =====================================
    # - "#SBATCH --partition=main"
    # - "#SBATCH --gres=gpu:rtx8000:2"
    # - "#SBATCH --mem=32G"
    # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< DEBUG
    - "#SBATCH --nodes=1"
    - "source /home/mila/m/marawan.gamal/scratch/tjdnet/.venv/bin/activate"

  gpu4long:   
    - "#!/bin/bash"
    - "#SBATCH --output=slurm/slurm-%j.out"
    - "#SBATCH --error=slurm/slurm-%j.err"   
    - "#SBATCH --partition=long"
    - "#SBATCH --gres=gpu:a100l:4"
    - "#SBATCH --cpus-per-task=12"
    - "#SBATCH --mem=128G"
    - "#SBATCH --nodes=1"
    - "#SBATCH --time=3:00:00"
    - "source /home/mila/m/marawan.gamal/scratch/tjdnet/.venv/bin/activate"

  cpu:
    - "#!/bin/bash"
    - "#SBATCH --output=slurm/slurm-%j.out"
    - "#SBATCH --error=slurm/slurm-%j.err"   
    - "#SBATCH --partition=unkillable-cpu"
    - "#SBATCH --mem=8G"
    - "source /home/mila/m/marawan.gamal/scratch/tjdnet/.venv/bin/activate"

  cpulong:   
    - "#!/bin/bash"
    - "#SBATCH --output=slurm/slurm-%j.out"
    - "#SBATCH --error=slurm/slurm-%j.err"   
    - "#SBATCH --partition=long"
    - "#SBATCH --mem=32G"
    - "#SBATCH --nodes=1"
    - "source /home/mila/m/marawan.gamal/scratch/tjdnet/.venv/bin/activate"


# MAIN: meta-llama/Llama-3.2-3B-Instruct
# DEBUG: distilbert/distilgpt2

# Root group executed in sequence
group:
  # aaa
  name: "main"
  type: parallel
  jobs:


  ################################################
  #                 BASE                         #  
  ################################################


    # w/o speculation
    - group:
        # aaa-bbb
        name: "spec:base"
        type: sequential
        jobs:
          #  Train on ShareGPT dataset
          #  1. Sweep over lr, rank, horizon
          - group:
              # aaa-bbb-XXXX
              name: "sweep"
              type: sweep
              preamble: cpulong
              sweep: # 3x4x5=60
                lr: [1e-3, 5e-4, 1e-4, 5e-5]
                model: ["meta-llama/Llama-3.2-3B-Instruct", "lmsys/vicuna-7b-v1.5"]
              sweep_template:  "python main.py train --accel_strategy fsdp --dataset sharegpt --model meta-llama/Llama-3.2-3B-Instruct --max_num_samples 1000 --epochs 2 --batch_size 8 --seq_len 128 --lr {lr} --model_head base --rank 1 --horizon 1 --use_memory_efficient_loss --slurm_job_id $SLURM_JOB_ID --group_id {group_id} --delete_ckpt"

          # 2. Tag best model for each (rank, horizon) combination
          - job:
              # aaa-bbb-bbb
              # Creates a <experiment_name>/.prospect file
              preamble: cpu
              command: "python main.py tag --group_by rank horizon --group_level 1 --group_id {group_id}"
              name: "tag"
      
          # 3. Run training/testing on best models
          - group:
              # aaa-bbb-ccc-XXXX
              type: loop
              loop_count: 1
              loop_type: parallel
              jobs: 
                - group:
                  # aaa-bbb-ccc-XXXX-aaa
                    type: sequential
                    jobs:
                      # Train best prospective model (on sharegpt) matching group_id @ group_level 1
                      # Creates a <experiment_name>/.best file
                      - job:
                          # aaa-bbb-ccc-XXXX-aaa-aaa
                          preamble: gpu4
                          command: "python main.py train --lookup --epochs 10  --group_level 1 --group_id {group_id}"
                          name: "tbest"

                      # Run expensive tests on best models
                      # 1. speculative
                      - job:
                          # aaa-bbb-ccc-XXXX-aaa-bbb
                          preamble: gpu4
                          command: "python main.py test --lookup --dataset gsm8k --group_level 1 --group_id {group_id} --gen_mode draft_multi_horizon" 
                          name: "test"


# e.g., single job
# jrun sbatch \
# --partition=short-unkillable \
# --gres=gpu:a100l:4 \
# --cpus-per-task=12 \
# --mem=128G \
# --nodes=1 \
# --time=3:00:00 \
# --output=slurm/slurm-%j.out \
# --error=slurm/slurm-%j.err \
# --wrap="source /home/mila/m/marawan.gamal/scratch/tjdnet/.venv/bin/activate; python main.py train --model_head cpo --rank 1"
