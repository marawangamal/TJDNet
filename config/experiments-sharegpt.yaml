# test.yaml
preambles:
  glong:   
    - "#!/bin/bash"
    - "#SBATCH --output=slurm/slurm-%j.out"
    - "#SBATCH --error=slurm/slurm-%j.err"   
    - "#SBATCH --partition=long"
    # MAIN >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
    - "#SBATCH --gres=gpu:a100l:2"
    - "#SBATCH --mem=128G"
    # =====================================
    # - "#SBATCH --gres=gpu:rtx8000:2"
    # - "#SBATCH --mem=32G"
    # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< DEBUG
    - "#SBATCH --cpus-per-task=12"
    - "#SBATCH --nodes=1"
    - "source /home/mila/m/marawan.gamal/scratch/tjdnet/.venv/bin/activate"

  cshort:
    - "#!/bin/bash"
    - "#SBATCH --output=slurm/slurm-%j.out"
    - "#SBATCH --error=slurm/slurm-%j.err"   
    - "#SBATCH --partition=long-cpu"
    - "#SBATCH --mem=32G"
    - "source /home/mila/m/marawan.gamal/scratch/tjdnet/.venv/bin/activate"

# MAIN: meta-llama/Llama-3.2-3B-Instruct
# DEBUG: distilbert/distilgpt2

# Root group executed in sequence
group:
  # aaa
  name: "main"
  type: parallel
  jobs:

  ################################################
  #                 BASE                         #  
  ################################################


    # w/o speculation
    - group:
        # aaa-bbb
        name: "sgpt:spec:base"
        type: sequential
        jobs:
          #  Train on ShareGPT dataset
          #  1. Sweep over lr, rank, horizon
          - group:
              # aaa-bbb-XXXX
              name: "sweep"
              type: sweep
              preamble: glong
              # ===== Main >>>>
              # sweep: 
              #   lr: [1e-3, 1e-4, 1e-5]
              #   init_mode: ["random"]
              # sweep_template:  "python main.py train --accel_strategy fsdp --dataset sharegpt --model lmsys/vicuna-7b-v1.5 --max_num_samples 500 --epochs 5 --batch_size 8 --seq_len 128 --lr {lr} --model_head base --rank 1 --horizon 1 --init_mode {init_mode} --use_memory_efficient_loss --slurm_job_id $SLURM_JOB_ID --group_id {group_id} --delete_ckpt --val_check_interval 1000"
              # ================
              sweep:
                lr: [1e-3]
                init_mode: ["random"]
              sweep_template:  "python main.py train --accel_strategy fsdp --dataset sharegpt --model lmsys/vicuna-7b-v1.5 --max_num_samples 10 --epochs 2 --batch_size 8 --seq_len 128 --lr {lr} --model_head base --rank 1 --horizon 1 --init_mode {init_mode} --use_memory_efficient_loss --slurm_job_id $SLURM_JOB_ID --group_id {group_id} --delete_ckpt"
              # <<<< Debug ====
          # 2. Tag best model for each (rank, horizon) combination
          - job:
              # aaa-bbb-bbb
              # Creates a <experiment_name>/.prospect file
              preamble: cshort
              command: "python main.py tag --group_by rank horizon init_mode --group_level 1 --group_id {group_id}"
              name: "tag"
      
          # 3. Run training/testing on best models
          - group:
              # aaa-bbb-ccc-XXXX
              type: loop
              loop_count: 1
              loop_type: parallel
              jobs: 
                - group:
                  # aaa-bbb-ccc-XXXX-aaa
                    type: sequential
                    jobs:
                      # Train best prospective model (on sharegpt) matching group_id @ group_level 1
                      # Creates a <experiment_name>/.best file
                      - job:
                          # aaa-bbb-ccc-XXXX-aaa-aaa
                          preamble: glong
                          # ===== Main >>>>
                          # command: "python main.py train --lookup --epochs 1 --max_num_samples 68000 --group_level 1 --group_id {group_id} --val_check_interval 1000"
                          # ================  
                          command: "python main.py train --lookup --epochs 1 --max_num_samples 1000 --group_level 1 --group_id {group_id} --val_check_interval 500"
                          # <<<< Debug =====
                          name: "tbest"

                      # Run expensive tests on best models
                      # 1. speculative
                      - job:
                          # aaa-bbb-ccc-XXXX-aaa-bbb
                          preamble: glong
                          command: "python main.py test --lookup --dataset gsm8k --template_mode few_shot --group_level 1 --group_id {group_id} --gen_mode speculative --delete_ckpt" 
                          name: "test"



  ################################################
  #                 CP                           #  
  ################################################


    # # w/o speculation
    # - group:
    #     # aaa-bbb
    #     name: "sgpt:spec:cp"
    #     type: sequential
    #     jobs:
    #       #  Train on ShareGPT dataset
    #       #  1. Sweep over lr, rank, horizon
    #       - group:
    #           # aaa-bbb-XXXX
    #           name: "sweep"
    #           type: sweep
    #           preamble: glong
    #           # ===== Main >>>>
    #           sweep: 
    #             lr: [1e-3, 5e-4, 1e-4, 5e-5, 1e-5]
    #             init_mode: ["random", "pretrained"]
    #             rank: [1, 8, 16]
    #             horizon: [2, 4, 8]
    #           sweep_template:  "python main.py train --accel_strategy single --dataset sharegpt --model lmsys/vicuna-7b-v1.5 --max_num_samples 5000 --epochs 2 --batch_size 8 --seq_len 128 --lr {lr} --model_head cp --rank {rank} --horizon {horizon} --init_mode {init_mode} --use_memory_efficient_loss --slurm_job_id $SLURM_JOB_ID --group_id {group_id} --delete_ckpt"
    #       # 2. Tag best model for each (rank, horizon) combination
    #       - job:
    #           # aaa-bbb-bbb
    #           # Creates a <experiment_name>/.prospect file
    #           preamble: cshort
    #           command: "python main.py tag --group_by rank horizon init_mode --group_level 1 --group_id {group_id}"
    #           name: "tag"
      
    #       # 3. Run training/testing on best models
    #       - group:
    #           # aaa-bbb-ccc-XXXX
    #           type: loop
    #           loop_count: 18
    #           loop_type: parallel
    #           jobs: 
    #             - group:
    #               # aaa-bbb-ccc-XXXX-aaa
    #                 type: sequential
    #                 jobs:
    #                   # Train best prospective model (on sharegpt) matching group_id @ group_level 1
    #                   # Creates a <experiment_name>/.best file
    #                   - job:
    #                       # aaa-bbb-ccc-XXXX-aaa-aaa
    #                       preamble: gpu4
    #                       command: "python main.py train --lookup --epochs 2 --max_num_samples 68000 --group_level 1 --group_id {group_id}"
    #                       name: "tbest"

    #                   # Run expensive tests on best models
    #                   # 1. speculative
    #                   - job:
    #                       # aaa-bbb-ccc-XXXX-aaa-bbb
    #                       preamble: gpu4
    #                       command: "python main.py test --lookup --dataset gsm8k --template_mode few_shot --group_level 1 --group_id {group_id} --gen_mode speculative" 
    #                       name: "test"





# e.g., single job
# jrun sbatch \
# --partition=short-unkillable \
# --gres=gpu:a100l:4 \
# --cpus-per-task=12 \
# --mem=128G \
# --nodes=1 \
# --time=3:00:00 \
# --output=slurm/slurm-%j.out \
# --error=slurm/slurm-%j.err \
# --wrap="source /home/mila/m/marawan.gamal/scratch/tjdnet/.venv/bin/activate; python main.py train --model_head cpo --rank 1"
