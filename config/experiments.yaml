# test.yaml
preambles:

  gpu1:   
    - "#!/bin/bash"
    - "#SBATCH --output=slurm/slurm-%j.out"
    - "#SBATCH --error=slurm/slurm-%j.err"   
    - "#SBATCH --partition=main"
    - "#SBATCH --gres=gpu:rtx8000:1"
    - "#SBATCH --mem=32G"
    - "#SBATCH --nodes=1"
    - "source /home/mila/m/marawan.gamal/scratch/tjdnet/.venv/bin/activate"

  cpub:   
    - "#!/bin/bash"
    - "#SBATCH --output=slurm/slurm-%j.out"
    - "#SBATCH --error=slurm/slurm-%j.err"   
    - "#SBATCH --partition=long"
    - "#SBATCH --mem=32G"
    - "#SBATCH --nodes=1"
    - "source /home/mila/m/marawan.gamal/scratch/tjdnet/.venv/bin/activate"

  gpu4:
    - "#!/bin/bash"
    - "#SBATCH --output=slurm/slurm-%j.out"
    - "#SBATCH --error=slurm/slurm-%j.err"   
    # MAIN >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
    - "#SBATCH --partition=short-unkillable"
    - "#SBATCH --gres=gpu:a100l:4"
    - "#SBATCH --mem=128G"
    - "#SBATCH --time=3:00:00"
    - "#SBATCH --cpus-per-task=12"
    # =====================================
    # - "#SBATCH --partition=main"
    # - "#SBATCH --gres=gpu:rtx8000:2"
    # - "#SBATCH --mem=32G"
    # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< DEBUG
    - "#SBATCH --nodes=1"
    - "source /home/mila/m/marawan.gamal/scratch/tjdnet/.venv/bin/activate"

  gpu4long:   
    - "#!/bin/bash"
    - "#SBATCH --output=slurm/slurm-%j.out"
    - "#SBATCH --error=slurm/slurm-%j.err"   
    - "#SBATCH --partition=long"
    - "#SBATCH --gres=gpu:a100l:4"
    - "#SBATCH --cpus-per-task=12"
    - "#SBATCH --mem=128G"
    - "#SBATCH --nodes=1"
    - "#SBATCH --time=3:00:00"
    - "source /home/mila/m/marawan.gamal/scratch/tjdnet/.venv/bin/activate"

  cpu:
    - "#!/bin/bash"
    - "#SBATCH --output=slurm/slurm-%j.out"
    - "#SBATCH --error=slurm/slurm-%j.err"   
    - "#SBATCH --partition=unkillable-cpu"
    - "#SBATCH --mem=8G"
    - "source /home/mila/m/marawan.gamal/scratch/tjdnet/.venv/bin/activate"




# MAIN: meta-llama/Llama-3.2-3B-Instruct
# DEBUG: distilbert/distilgpt2

# Root group executed in sequence
group:
  # aaa
  name: "main"
  type: parallel
  jobs:

  ################################################
  #      SPLIT SEARCH SPACE ACROSS PARTITIONS    #  
  ################################################
   
    # # w/o speculation
    # - group:
    #     # aaa-bbb
    #     name: "cp"
    #     type: sequential
    #     jobs:
    #       #  1. Sweep over lr, rank, horizon
    #       # aaa-bbb-aaa-XXXX
    #       - group:
    #           type: parallel
    #           name: "sweep"
    #           jobs:
    #           # MAIN >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
    #             - group:
    #                 type: sweep
    #                 preamble: cpub
    #                 sweep: # 3x1x5=15
    #                   lr: [1e-3, 5e-4, 1e-4, 5e-5]
    #                   rank: [2, 8, 16]
    #                   horizon: [2, 4, 8]
    #                 sweep_template:  "python main.py train --accel_strategy fsdp --dataset gsm8k --model meta-llama/Llama-3.2-3B-Instruct --epochs 5 --batch_size 8 --seq_len 128 --lr {lr} --model_head cp --rank {rank} --horizon {horizon} --use_memory_efficient_loss --slurm_job_id $SLURM_JOB_ID --group_id {group_id} --delete_ckpt"
    #             # # =====================================                
    #             # - group:
    #             #     type: sweep
    #             #     preamble: gpu4
    #             #     sweep: # 3x4x5=60
    #             #       lr: [1e-4, 1e-5]
    #             #       rank: [8]
    #             #       horizon: [2]
    #             #     sweep_template:  "python main.py train --accel_strategy fsdp --dataset gsm8k --model distilbert/distilgpt2 --epochs 2 --max_num_samples 1 --batch_size 8 --seq_len 128 --lr {lr} --model_head cp --rank {rank} --horizon {horizon} --use_memory_efficient_loss --slurm_job_id $SLURM_JOB_ID --group_id {group_id} --delete_ckpt"
    #             # # <<<<<<<<<<<<<<<<<<<<<<<<<<<<< DEBUG

    #       # 2. Tag best model for each (rank, horizon) combination
    #       # aaa-bbb-bbb
    #       - job:
    #           # Creates a <experiment_name>/.prospect file
    #           preamble: cpu
    #           command: "python main.py tag --group_by rank horizon --group_level 1 --group_id {group_id}"
    #           name: "tag"
      

    #       # 3. Run training/testing on best models
    #       # aaa-bbb-ccc-XXXX
    #       - group:
    #           type: loop
    #           # >>> MAIN
    #           loop_count: 9
    #           # # ====
    #           # loop_count: 2
    #           # # <<< DEBUG
    #           jobs: 
    #             # aaa-bbb-ccc-aaa
    #             # Creates a <experiment_name>/.best file
    #             - job:
    #                 preamble: gpu4long
    #                 command: "python main.py train --lookup --epochs 10  --group_level 1 --group_id {group_id}"
    #                 name: "tbest"

    #             # Run expensive test on best models
    #             # aaa-bbb-ccc-bbb
    #             - job:
    #                 preamble: gpu4
    #                 command: "python main.py test --lookup --group_level 1 --group_id {group_id} --delete_ckpt" 
    #                 name: "test"

    # w/ speculation
    - group:
        # aaa-bbb
        name: "cp"
        type: sequential
        jobs:
          #  1. Sweep over lr, rank, horizon
          # aaa-bbb-aaa-XXXX
          - group:
              type: parallel
              name: "sweep"
              jobs:
                # MAIN >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
                # - group:
                #     type: sweep
                #     preamble: gpu4long
                #     sweep: # 3x4x5=60
                #       lr: [1e-3, 5e-4, 1e-4, 5e-5]
                #       rank: [2, 8, 16]
                #       horizon: [2, 4, 8]
                #     sweep_template:  "python main.py train --accel_strategy fsdp --dataset gsm8k --model meta-llama/Llama-3.2-3B-Instruct --epochs 5 --batch_size 8 --seq_len 128 --lr {lr} --model_head cp --rank {rank} --horizon {horizon} --use_memory_efficient_loss --slurm_job_id $SLURM_JOB_ID --group_id {group_id} --loss_mode joint --delete_ckpt"
                # # =====================================                
                - group:
                    type: sweep
                    preamble: cpub
                    sweep: # 3x4x5=60
                      lr: [1e-3, 1e-4]
                      rank: [8]
                      horizon: [2]
                    sweep_template:  "python main.py train --accel_strategy fsdp --dataset gsm8k --model meta-llama/Llama-3.2-3B-Instruct --epochs 5 --batch_size 8 --seq_len 128 --lr {lr} --model_head cp --rank {rank} --horizon {horizon} --use_memory_efficient_loss --slurm_job_id $SLURM_JOB_ID --group_id {group_id} --loss_mode joint --delete_ckpt"
                # # <<<<<<<<<<<<<<<<<<<<<<<<<<<<< DEBUG

          # 2. Tag best model for each (rank, horizon) combination
          # aaa-bbb-bbb
          - job:
              # Creates a <experiment_name>/.prospect file
              preamble: cpu
              command: "python main.py tag --group_by rank horizon --group_level 1 --group_id {group_id}"
              name: "tag"
      

          # 3. Run training/testing on best models
          # aaa-bbb-ccc-XXXX
          - group:
              type: loop
              # >>> MAIN
              # loop_count: 9
              # # ====
              loop_count: 1
              # # <<< DEBUG
              jobs: 
                # aaa-bbb-ccc-aaa
                # Creates a <experiment_name>/.best file
                - job:
                    preamble: gpu4
                    command: "python main.py train --lookup --epochs 10  --group_level 1 --group_id {group_id}"
                    name: "tbest"

                # Run expensive test on best models
                # aaa-bbb-ccc-bbb
                - job:
                    preamble: gpu4
                    command: "python main.py test --lookup --group_level 1 --group_id {group_id} --delete_ckpt --gen_mode mixed" 
                    name: "test"


  ################################################
  #                 BASE                         #  
  ################################################

    # # w/o speculation
    # - group:
    #     # aaa-aaa
    #     name: "base"
    #     type: sequential
    #     jobs:
    #       #  1. Sweep over lr, rank, horizon
    #       # aaa-aaa-aaa-XXXX
    #       - group:
    #           type: parallel
    #           name: "sweep"
    #           jobs:
    #             - group:
    #                 type: sweep
    #                 preamble: gpu4long
    #                 sweep: # 3x4x5=60
    #                   lr: [5e-4, 1e-4, 5e-5]
    #                 sweep_template:  "python main.py train --accel_strategy fsdp --dataset gsm8k --model meta-llama/Llama-3.2-3B-Instruct --epochs 5 --batch_size 8 --seq_len 128 --lr {lr} --model_head base --rank 1 --horizon 1 --slurm_job_id $SLURM_JOB_ID --group_id {group_id} --delete_ckpt"

    #       # 2. Tag best model for each (rank, horizon) combination
    #       # aaa-aaa-bbb
    #       - job:
    #           # Creates a <experiment_name>/.prospect file
    #           preamble: cpu
    #           command: "python main.py tag --group_by rank horizon --group_level 1 --group_id {group_id}"
    #           name: "tag"
      

    #       # 3. Run training/testing on best models
    #       # aaa-aaa-ccc-XXXX
    #       - group:
    #           type: loop
    #           loop_count: 1
    #           jobs: 
    #             # aaa-aaa-ccc-aaa
    #             # Creates a <experiment_name>/.best file
    #             - job:
    #                 preamble: gpu4long
    #                 command: "python main.py train --lookup --epochs 10  --group_level 1 --group_id {group_id}"
    #                 name: "tbest"

    #             # Run expensive test on best models
    #             # aaa-aaa-ccc-bbb
    #             - job:
    #                 preamble: gpu4
    #                 command: "python main.py test --lookup --group_level 1 --group_id {group_id} --delete_ckpt" 
    #                 name: "test"

    # # w/ speculation
    # - group:
    #     # aaa-aaa
    #     name: "base"
    #     type: sequential
    #     jobs:
    #       #  1. Sweep over lr, rank, horizon
    #       # aaa-aaa-aaa-XXXX
    #       - group:
    #           type: parallel
    #           name: "sweep"
    #           jobs:
    #             - group:
    #                 type: sweep
    #                 preamble: gpu4long
    #                 sweep: # 3x4x5=60
    #                   lr: [5e-4, 1e-4, 5e-5]
    #                 sweep_template:  "python main.py train --accel_strategy fsdp --dataset gsm8k --model meta-llama/Llama-3.2-3B-Instruct --epochs 5 --batch_size 8 --seq_len 128 --lr {lr} --model_head base --rank 1 --horizon 1 --slurm_job_id $SLURM_JOB_ID --group_id {group_id} --loss_mode joint --delete_ckpt"

    #       # 2. Tag best model for each (rank, horizon) combination
    #       # aaa-aaa-bbb
    #       - job:
    #           # Creates a <experiment_name>/.prospect file
    #           preamble: cpu
    #           command: "python main.py tag --group_by rank horizon --group_level 1 --group_id {group_id}"
    #           name: "tag"
      

    #       # 3. Run training/testing on best models
    #       # aaa-aaa-ccc-XXXX
    #       - group:
    #           type: loop
    #           loop_count: 1
    #           jobs: 
    #             # aaa-aaa-ccc-aaa
    #             # Creates a <experiment_name>/.best file
    #             - job:
    #                 preamble: gpu4long
    #                 command: "python main.py train --lookup --epochs 10  --group_level 1 --group_id {group_id}"
    #                 name: "tbest"

    #             # Run expensive test on best models
    #             # aaa-aaa-ccc-bbb
    #             - job:
    #                 preamble: gpu4
    #                 command: "python main.py test --lookup --group_level 1 --group_id {group_id} --delete_ckpt --gen_mode mixed" 
    #                 name: "test"

# e.g., single job
# jrun sbatch --partition=short-unkillable --gres=gpu:a100l:4 --cpus-per-task=12 --mem=128G --nodes=1 --time=3:00:00 --output=slurm/slurm-%j.out --error=slurm/slurm-%j.err --wrap="source /home/mila/m/marawan.gamal/scratch/tjdnet/.venv/bin/activate; python main.py test --lookup --group_level 1 --group_id 676435-794227-459408-869771 --delete_ckpt"
