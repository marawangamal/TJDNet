# test.yaml
preambles:

  gpu4:
    - "#!/bin/bash"
    - "#SBATCH --output=slurm/slurm-%j.out"
    - "#SBATCH --error=slurm/slurm-%j.err"   
    # MAIN >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
    - "#SBATCH --partition=short-unkillable"
    - "#SBATCH --gres=gpu:a100l:4"
    - "#SBATCH --mem=128G"
    - "#SBATCH --time=3:00:00"
    - "#SBATCH --cpus-per-task=12"
    # =====================================
    # - "#SBATCH --partition=main"
    # - "#SBATCH --gres=gpu:rtx8000:2"
    # - "#SBATCH --mem=32G"
    # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< DEBUG
    - "#SBATCH --nodes=1"
    - "source /home/mila/m/marawan.gamal/scratch/tjdnet/.venv/bin/activate"

  gpu4long:   
    - "#!/bin/bash"
    - "#SBATCH --output=slurm/slurm-%j.out"
    - "#SBATCH --error=slurm/slurm-%j.err"   
    - "#SBATCH --partition=long"
    - "#SBATCH --gres=gpu:a100l:4"
    - "#SBATCH --cpus-per-task=12"
    - "#SBATCH --mem=128G"
    - "#SBATCH --nodes=1"
    - "#SBATCH --time=3:00:00"
    - "source /home/mila/m/marawan.gamal/scratch/tjdnet/.venv/bin/activate"

  cpu:
    - "#!/bin/bash"
    - "#SBATCH --output=slurm/slurm-%j.out"
    - "#SBATCH --error=slurm/slurm-%j.err"   
    - "#SBATCH --partition=unkillable-cpu"
    - "#SBATCH --mem=8G"
    - "source /home/mila/m/marawan.gamal/scratch/tjdnet/.venv/bin/activate"

  cpulong:   
    - "#!/bin/bash"
    - "#SBATCH --output=slurm/slurm-%j.out"
    - "#SBATCH --error=slurm/slurm-%j.err"   
    - "#SBATCH --partition=long"
    - "#SBATCH --mem=32G"
    - "#SBATCH --nodes=1"
    - "source /home/mila/m/marawan.gamal/scratch/tjdnet/.venv/bin/activate"


# MAIN: meta-llama/Llama-3.2-3B-Instruct
# DEBUG: distilbert/distilgpt2

# Root group executed in sequence
group:
  # aaa
  name: "main"
  type: parallel
  jobs:

  ################################################
  #                     CP                       #  
  ################################################
   
    # # CP  w/o speculation
    # - group:
    #     # aaa-bbb
    #     name: "main:cp"
    #     type: sequential
    #     jobs:
    #       #  1. Sweep over lr, rank, horizon
    #       # aaa-bbb-aaa-XXXX
    #       - group:
    #           type: parallel
    #           name: "sweep"
    #           jobs:
    #           # MAIN >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
    #             - group:
    #                 type: sweep
    #                 preamble: cpulong
    #                 sweep: # 3x1x5=15
    #                   lr: [1e-3, 5e-4, 1e-4, 5e-5]
    #                   rank: [2, 8, 16]
    #                   horizon: [2, 4, 8]
    #                 sweep_template:  "python main.py train --accel_strategy fsdp --dataset gsm8k --model meta-llama/Llama-3.2-3B-Instruct --epochs 5 --batch_size 8 --seq_len 128 --lr {lr} --model_head cp --rank {rank} --horizon {horizon} --use_memory_efficient_loss --slurm_job_id $SLURM_JOB_ID --group_id {group_id} --delete_ckpt"
    #             # # =====================================                
    #             # - group:
    #             #     type: sweep
    #             #     preamble: gpu4
    #             #     sweep: # 3x4x5=60
    #             #       lr: [1e-4, 1e-5]
    #             #       rank: [8]
    #             #       horizon: [2]
    #             #     sweep_template:  "python main.py train --accel_strategy fsdp --dataset gsm8k --model distilbert/distilgpt2 --epochs 2 --max_num_samples 1 --batch_size 8 --seq_len 128 --lr {lr} --model_head cp --rank {rank} --horizon {horizon} --use_memory_efficient_loss --slurm_job_id $SLURM_JOB_ID --group_id {group_id} --delete_ckpt"
    #             # # <<<<<<<<<<<<<<<<<<<<<<<<<<<<< DEBUG

    #       # 2. Tag best model for each (rank, horizon) combination
    #       # aaa-bbb-bbb
    #       - job:
    #           # Creates a <experiment_name>/.prospect file
    #           preamble: cpu
    #           command: "python main.py tag --group_by rank horizon --group_level 1 --group_id {group_id}"
    #           name: "tag"
      

    #       # 3. Run training/testing on best models
    #       # aaa-bbb-ccc-XXXX
    #       - group:
    #           type: loop
    #           # >>> MAIN
    #           loop_count: 9
    #           loop_type: parallel
    #           # # ====
    #           # loop_count: 2
    #           # # <<< DEBUG
    #           jobs: 
    #             - group:
    #                 type: sequential
    #                 name: "tbest"
    #                 jobs:
    #                   # aaa-bbb-ccc-aaa-aaa
    #                   # Creates a <experiment_name>/.best file
    #                   - job:
    #                       preamble: gpu4long
    #                       command: "python main.py train --lookup --epochs 10  --group_level 1 --group_id {group_id}"
    #                       name: "tbest"

    #                   # Run expensive test on best models
    #                   # aaa-bbb-ccc-aaa-bbb
    #                   - job:
    #                       preamble: gpu4
    #                       command: "python main.py test --lookup --group_level 1 --group_id {group_id} --delete_ckpt --gen_mode draft_multi_horizon"
    #                       name: "test"


    # CP w/ speculation
    # - group:
    #     # aaa-bbb
    #     name: "spec:cp"
    #     type: sequential
    #     jobs:
    #       #  1. Sweep over lr, rank, horizon
    #       # MAIN >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
    #       - group:
    #           type: sweep
    #           preamble: gpu4long
    #           sweep: # 3x4x5=60
    #             lr: [1e-2, 5e-3, 1e-3, 5e-4, 1e-4, 1e-5]
    #             rank: [2, 8, 16, 32]
    #             horizon: [2, 4, 8]
    #           sweep_template:  "python main.py train --accel_strategy fsdp --dataset gsm8k --model meta-llama/Llama-3.2-3B-Instruct --epochs 2 --batch_size 8 --seq_len 128 --lr {lr} --model_head cp --rank {rank} --horizon {horizon} --use_memory_efficient_loss --slurm_job_id $SLURM_JOB_ID --group_id {group_id} --loss_mode joint --delete_ckpt"
    #       # # =====================================                
    #       # - group:
    #       #     type: sweep
    #       #     preamble: gpu4long
    #       #     sweep: # 3x4x5=60
    #       #       lr: [1e-3, 1e-4]
    #       #       rank: [8]
    #       #       horizon: [2, 4]
    #       #     sweep_template:  "python main.py train --accel_strategy fsdp --dataset gsm8k --model meta-llama/Llama-3.2-3B-Instruct --epochs 2 --batch_size 8 --seq_len 128 --lr {lr} --model_head cp --rank {rank} --horizon {horizon} --use_memory_efficient_loss --slurm_job_id $SLURM_JOB_ID --group_id {group_id} --loss_mode joint --delete_ckpt"
    #       # # <<<<<<<<<<<<<<<<<<<<<<<<<<<<< DEBUG

    #       # 2. Tag best model for each (rank, horizon) combination
    #       - job:
    #           # aaa-bbb-bbb
    #           # Creates a <experiment_name>/.prospect file
    #           preamble: cpu
    #           command: "python main.py tag --group_by rank horizon --group_level 1 --group_id {group_id}"
    #           name: "tag"
      

    #       # 3. Run training/testing on best models
    #       - group:
    #           # aaa-bbb-ccc-XXXX
    #           type: loop
    #           # >>> MAIN
    #           loop_count: 12
    #           # ====
    #           # loop_count: 2
    #           # <<< DEBUG
    #           loop_type: parallel
    #           jobs: 
    #             - group:
    #               # aaa-bbb-ccc-XXXX-aaa
    #                 type: sequential
    #                 jobs:
    #                   # Train best prospective model matching group_id @ group_level 1
    #                   # Creates a <experiment_name>/.best file
    #                   - job:
    #                       # aaa-bbb-ccc-XXXX-aaa-aaa
    #                       preamble: gpu4
    #                       command: "python main.py train --lookup --epochs 10  --group_level 1 --group_id {group_id} --idx {loop_idx}"
    #                       name: "tbest"

    #                   # Run expensive tests on best models
    #                   # 1. speculative
    #                   - job:
    #                       # aaa-bbb-ccc-XXXX-aaa-bbb
    #                       preamble: gpu4
    #                       command: "python main.py test --lookup --group_level 1 --group_id {group_id} --idx {loop_idx} --gen_mode speculative" 
    #                       name: "test"

    #                   # 2. draft_multi_horizon
    #                   - job:
    #                       # aaa-bbb-ccc-XXXX-aaa-ccc
    #                       preamble: gpu4
    #                       command: "python main.py test --lookup --group_level 1 --group_id {group_id} --idx {loop_idx} --gen_mode draft_multi_horizon" 
    #                       name: "test"

    #                   # 3. base_multi_horizon
    #                   - job:
    #                       # aaa-bbb-ccc-XXXX-aaa-ddd
    #                       preamble: gpu4
    #                       command: "python main.py test --lookup --group_level 1 --group_id {group_id} --idx {loop_idx} --delete_ckpt --gen_mode base_multi_horizon" 
    #                       name: "test"



    # MTP w/ speculation
    - group:
        # aaa-bbb
        name: "spec:cpr1"
        type: sequential
        jobs:
          #  1. Sweep over lr, rank, horizon
          - group:
              type: sweep
              preamble: gpu4long
              sweep: # 3x4x5=60
                lr: [1e-2, 5e-3, 1e-3, 5e-4, 1e-4, 1e-5]
                rank: [1]
                horizon: [2, 4, 8]
              sweep_template:  "python main.py train --accel_strategy fsdp --dataset gsm8k --model meta-llama/Llama-3.2-3B-Instruct --epochs 2 --batch_size 8 --seq_len 128 --lr {lr} --model_head cp --rank {rank} --horizon {horizon} --use_memory_efficient_loss --slurm_job_id $SLURM_JOB_ID --group_id {group_id} --loss_mode joint --delete_ckpt"

          # 2. Tag best model for each (rank, horizon) combination
          - job:
              # aaa-bbb-bbb
              # Creates a <experiment_name>/.prospect file
              preamble: cpu
              command: "python main.py tag --group_by rank horizon --group_level 1 --group_id {group_id}"
              name: "tag"
      
          # 3. Run training/testing on best models
          - group:
              # aaa-bbb-ccc-XXXX
              type: loop
              loop_count: 3
              loop_type: parallel
              jobs: 
                - group:
                  # aaa-bbb-ccc-XXXX-aaa
                    type: sequential
                    jobs:
                      # Train best prospective model matching group_id @ group_level 1
                      # Creates a <experiment_name>/.best file
                      - job:
                          # aaa-bbb-ccc-XXXX-aaa-aaa
                          preamble: gpu4
                          command: "python main.py train --lookup --epochs 10  --group_level 1 --group_id {group_id} --idx {loop_idx}"
                          name: "tbest"

                      # Run expensive tests on best models
                      # 1. speculative
                      - job:
                          # aaa-bbb-ccc-XXXX-aaa-bbb
                          preamble: gpu4
                          command: "python main.py test --lookup --group_level 1 --group_id {group_id} --idx {loop_idx} --gen_mode speculative" 
                          name: "test"

                      # 2. draft_multi_horizon
                      - job:
                          # aaa-bbb-ccc-XXXX-aaa-ccc
                          preamble: gpu4
                          command: "python main.py test --lookup --group_level 1 --group_id {group_id} --idx {loop_idx} --gen_mode draft_multi_horizon" 
                          name: "test"

                      # 3. base_multi_horizon
                      - job:
                          # aaa-bbb-ccc-XXXX-aaa-ddd
                          preamble: gpu4
                          command: "python main.py test --lookup --group_level 1 --group_id {group_id} --idx {loop_idx} --delete_ckpt --gen_mode base_multi_horizon" 
                          name: "test"


  ################################################
  #                 BASE                         #  
  ################################################


    # w/ speculation
    # - group:
    #     # aaa-bbb
    #     name: "spec:base"
    #     type: sequential
    #     jobs:
    #       #  1. Sweep over lr, rank, horizon
    #       - group:
    #           # aaa-bbb-XXXX
    #           name: "sweep"
    #           type: sweep
    #           preamble: cpulong
    #           sweep: # 3x4x5=60
    #             lr: [1e-3, 5e-4, 1e-4, 5e-5]
    #           sweep_template:  "python main.py train --accel_strategy fsdp --dataset gsm8k --model meta-llama/Llama-3.2-3B-Instruct --epochs 2 --batch_size 8 --seq_len 128 --lr {lr} --model_head base --rank 1 --horizon 1 --use_memory_efficient_loss --slurm_job_id $SLURM_JOB_ID --group_id {group_id} --loss_mode joint --delete_ckpt"

    #       # 2. Tag best model for each (rank, horizon) combination
    #       - job:
    #           # aaa-bbb-bbb
    #           # Creates a <experiment_name>/.prospect file
    #           preamble: cpu
    #           command: "python main.py tag --group_by rank horizon --group_level 1 --group_id {group_id}"
    #           name: "tag"
      
    #       # 3. Run training/testing on best models
    #       - group:
    #           # aaa-bbb-ccc-XXXX
    #           type: loop
    #           loop_count: 1
    #           loop_type: parallel
    #           jobs: 
    #             - group:
    #               # aaa-bbb-ccc-XXXX-aaa
    #                 type: sequential
    #                 jobs:
    #                   # Train best prospective model matching group_id @ group_level 1
    #                   # Creates a <experiment_name>/.best file
    #                   - job:
    #                       # aaa-bbb-ccc-XXXX-aaa-aaa
    #                       preamble: gpu4
    #                       command: "python main.py train --lookup --epochs 10  --group_level 1 --group_id {group_id}"
    #                       name: "tbest"

    #                   # Run expensive tests on best models
    #                   # 1. speculative
    #                   - job:
    #                       # aaa-bbb-ccc-XXXX-aaa-bbb
    #                       preamble: gpu4
    #                       command: "python main.py test --lookup --group_level 1 --group_id {group_id} --gen_mode speculative" 
    #                       name: "test"

    #                   # 2. draft_multi_horizon
    #                   - job:
    #                       # aaa-bbb-ccc-XXXX-aaa-ccc
    #                       preamble: gpu4
    #                       command: "python main.py test --lookup --group_level 1 --group_id {group_id} --gen_mode draft_multi_horizon" 
    #                       name: "test"

    #                   # 3. base_multi_horizon
    #                   - job:
    #                       # aaa-bbb-ccc-XXXX-aaa-ddd
    #                       preamble: gpu4
    #                       command: "python main.py test --lookup --group_level 1 --group_id {group_id} --delete_ckpt --gen_mode base_multi_horizon" 
    #                       name: "test"

# e.g., single job
# jrun sbatch \
# --partition=short-unkillable \
# --gres=gpu:a100l:4 \
# --cpus-per-task=12 \
# --mem=128G \
# --nodes=1 \
# --time=3:00:00 \
# --output=slurm/slurm-%j.out \
# --error=slurm/slurm-%j.err \
# --wrap="source /home/mila/m/marawan.gamal/scratch/tjdnet/.venv/bin/activate; python main.py fit --config config/config.yaml --model.model lmsys/vicuna-7b-v1.5 --trainer.max_epochs 4  --auto_lr_find --data.batch_size 8 --trainer.strategy ddp_find_unused_parameters_true"
