# experiments-dcomplexity.yaml
# This file contains experiments to run on datasets with different empirical ranks 
# using MTP (Multi-Task Prediction) and STP (Single-Task Prediction) approaches
preambles:
  glong:   
    - "#!/bin/bash"
    - "#SBATCH --output=slurm/slurm-%j.out"
    - "#SBATCH --error=slurm/slurm-%j.err"   
    - "#SBATCH --partition=long"
    - "#SBATCH --gres=gpu:a100l:1"
    - "#SBATCH --mem=128G"
    - "#SBATCH --cpus-per-task=12"
    - "#SBATCH --nodes=1"
    - "source /home/mila/m/marawan.gamal/scratch/tjdnet/.venv/bin/activate"

  cshort:
    - "#!/bin/bash"
    - "#SBATCH --output=slurm/slurm-%j.out"
    - "#SBATCH --error=slurm/slurm-%j.err"   
    - "#SBATCH --partition=long-cpu"
    - "#SBATCH --mem=32G"
    - "source /home/mila/m/marawan.gamal/scratch/tjdnet/.venv/bin/activate"

# MAIN: meta-llama/Llama-3.2-3B-Instruct
# DEBUG: distilbert/distilgpt2

# Root group executed in sequence
group:
  # aaa
  name: "main"
  type: parallel
  jobs:

  ################################################
  #                 BASE                         #  
  ################################################


    # # w/o speculation
    # - group:
    #     # aaa-bbb
    #     name: "sharegpt:base"
    #     type: sequential
    #     jobs:
    #       #  Train on ShareGPT dataset
    #       #  1. Sweep over lr, rank, horizon
    #       - group:
    #           # aaa-bbb-XXXX
    #           name: "sweep"
    #           type: sweep
    #           preamble: glong
    #           sweep: 
    #             lr: [1e-3]
    #             max_num_samples: [5000, 10000, 68000]
    #           sweep_template:  "python main.py fit --config config/config.yaml --trainer.max_epochs 1 --model.model lmsys/vicuna-7b-v1.5 --model.hidden_dim 4096 --model.horizon 1 --model.rank 1 --model.positivity_func safe_exp --data.batch_size 8 --data.dataset sharegpt --data.max_num_samples {max_num_samples} --auto_lr_find"


  ################################################
  #                 CP                           #  
  ################################################

    - group:
        # aaa-bbb
        name: "dcomplexity::cp"
        type: sequential
        jobs:
          #  Train on ShareGPT dataset
          #  1. Sweep over lr, rank, horizon
          - group:
              # aaa-bbb-XXXX
              name: "sweep"
              type: sweep
              preamble: glong
              sweep: 
                lr: [1e-3]
                max_num_samples: [10000]
                positivity_func: [sigmoid]
                horizon: [2]
                rank: [8]
                dataset: [aqua_rat, wikitext2, reddit, sst2]
              sweep_template:  "python main.py fit --config config/config.yaml --trainer.max_epochs 10 --trainer.gradient_clip_val 1.0 --model.model meta-llama/Llama-2-7b-chat-hf --model.horizon {horizon} --model.rank {rank} --model.model_head cp --model.positivity_func {positivity_func} --data.batch_size 8 --data.dataset {dataset} --data.max_num_samples {max_num_samples} --auto_lr_find"


    - group:
        # aaa-bbb
        name: "dcomplexity::cp"
        type: sequential
        jobs:
          #  Train on ShareGPT dataset
          #  1. Sweep over lr, rank, horizon
          - group:
              # aaa-bbb-XXXX
              name: "sweep"
              type: sweep
              preamble: glong
              sweep: 
                lr: [1e-3]
                max_num_samples: [10000]
                positivity_func: [sigmoid]
                horizon: [2]
                rank: [8]
                dataset: [aqua_rat, wikitext2, reddit, sst2]
              sweep_template:  "python main.py fit --config config/config.yaml --trainer.max_epochs 10 --trainer.gradient_clip_val 1.0 --model.model meta-llama/Llama-2-7b-chat-hf --model.horizon {horizon} --model.rank {rank} --model.model_head multihead --model.positivity_func {positivity_func} --data.batch_size 8 --data.dataset {dataset} --data.max_num_samples {max_num_samples} --auto_lr_find"



    - group:
        # aaa-bbb
        name: "dcomplexity::base"
        type: sequential
        jobs:
          #  Train on ShareGPT dataset
          #  1. Sweep over lr, rank, horizon
          - group:
              # aaa-bbb-XXXX
              name: "sweep"
              type: sweep
              preamble: glong
              sweep: 
                lr: [1e-3]
                max_num_samples: [10000]
                positivity_func: [sigmoid]
                horizon: [2]
                rank: [8]
                dataset: [aqua_rat, wikitext2, reddit, sst2]
              sweep_template:  "python main.py fit --config config/config.yaml --trainer.max_epochs 10 --trainer.gradient_clip_val 1.0 --model.model meta-llama/Llama-2-7b-chat-hf --model.horizon {horizon} --model.rank {rank} --model.model_head stp --model.positivity_func {positivity_func} --data.batch_size 8 --data.dataset {dataset} --data.max_num_samples {max_num_samples} --auto_lr_find"

