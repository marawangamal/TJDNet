#!/bin/bash

# Baseline (random & pretrained)
sbatch scripts/slurm/large-unkillable.slurm accelerate launch --multi_gpu train.py --epochs 50 --logging_strategy epoch --logging_steps 1 --eval_strategy epoch --eval_steps 1 --generate_strategy epoch --tokenizer_type word --num_layers 1 --dropout 0 --top_k 200 --num_beams 1 --batch_size 4 --seq_len 128 --dataset gsm8k --model_type llama7b --model_head base --horizon 1 --horizon_eval 1 --rank 1 --init_method random --lr 1e-5 --train_mode lora --lora_rank 32 --compute_acc
# sbatch scripts/slurm/large-unkillable.slurm accelerate launch --multi_gpu train.py --epochs 50 --logging_strategy epoch --logging_steps 1 --eval_strategy epoch --eval_steps 1 --generate_strategy epoch --tokenizer_type word --num_layers 1 --dropout 0 --top_k 200 --num_beams 1 --batch_size 4 --seq_len 128 --dataset gsm8k --model_type llama7b --model_head base --horizon 1 --horizon_eval 1 --rank 1 --init_method pretrained --lr 1e-5 --train_mode lora --lora_rank 32 --compute_acc


# CP (rank=2,4,8 & horizon=2,4)
sbatch scripts/slurm/large-unkillable.slurm accelerate launch --use_fsdp --config_file accelrate/conf/fsdp_4gpus.yaml train.py --epochs 50 --logging_strategy epoch --logging_steps 1 --eval_strategy epoch --eval_steps 1 --generate_strategy epoch --tokenizer_type word --num_layers 2 --hidden_dim 768 --dropout 0 --top_k 200 --num_beams 1 --batch_size 8 --seq_len 128 --dataset gsm8k --model_type llama7b --model_head cp --horizon 2 --horizon_eval 2 --rank 2 --lr 1e-5 --train_mode lora --lora_rank 32
sbatch scripts/slurm/large-unkillable.slurm accelerate launch --use_fsdp --config_file accelrate/conf/fsdp_4gpus.yaml train.py --epochs 50 --logging_strategy epoch --logging_steps 1 --eval_strategy epoch --eval_steps 1 --generate_strategy epoch --tokenizer_type word --num_layers 2 --hidden_dim 768 --dropout 0 --top_k 200 --num_beams 1 --batch_size 8 --seq_len 128 --dataset gsm8k --model_type llama7b --model_head cp --horizon 2 --horizon_eval 2 --rank 4 --lr 1e-5 --train_mode lora --lora_rank 32
sbatch scripts/slurm/large-unkillable.slurm accelerate launch --use_fsdp --config_file accelrate/conf/fsdp_4gpus.yaml train.py --epochs 50 --logging_strategy epoch --logging_steps 1 --eval_strategy epoch --eval_steps 1 --generate_strategy epoch --tokenizer_type word --num_layers 2 --hidden_dim 768 --dropout 0 --top_k 200 --num_beams 1 --batch_size 8 --seq_len 128 --dataset gsm8k --model_type llama7b --model_head cp --horizon 2 --horizon_eval 2 --rank 8 --lr 1e-5 --train_mode lora --lora_rank 32
sbatch scripts/slurm/large-unkillable.slurm accelerate launch --use_fsdp --config_file accelrate/conf/fsdp_4gpus.yaml train.py --epochs 50 --logging_strategy epoch --logging_steps 1 --eval_strategy epoch --eval_steps 1 --generate_strategy epoch --tokenizer_type word --num_layers 2 --hidden_dim 768 --dropout 0 --top_k 200 --num_beams 1 --batch_size 8 --seq_len 128 --dataset gsm8k --model_type llama7b --model_head cp --horizon 2 --horizon_eval 2 --rank 16 --lr 1e-5 --train_mode lora --lora_rank 32



# Scratch
# python train.py --epochs 50 --logging_strategy epoch --logging_steps 1 --eval_strategy epoch --eval_steps 1 --generate_strategy epoch --tokenizer_type word --num_layers 1 --dropout 0 --top_k 200 --num_beams 1 --batch_size 1 --seq_len 128 --dataset gsm8k --model_type llama7b --model_head base --horizon 1 --horizon_eval 1 --rank 1 --init_method random --lr 1e-5 --train_mode lora --lora_rank 32 --compute_acc
